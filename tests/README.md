# HEAL Onboarding System Test Suite

Comprehensive test suite for the HEAL onboarding system, providing thorough validation of all components, integration scenarios, and performance characteristics.

## ðŸ“‹ Test Overview

The test suite is organized into several categories:

### ðŸ§ª Unit Tests
- **test_user_state_tracker.py**: User state management, preferences, and onboarding progress
- **test_smart_tip_system.py**: Intelligent tip selection, context awareness, and rotation
- **test_recommendation_engine.py**: Recommendation generation, behavior analysis, and lifecycle
- **test_tutorial_system.py**: Interactive tutorials, step validation, and progress tracking

### ðŸ”— Integration Tests
- **test_onboarding_system.py**: Complete system integration and component interactions
- **test_integration_scenarios.py**: Real-world user journeys and workflows

### âš¡ Performance Tests
- **test_performance_benchmarks.py**: Performance benchmarks, memory usage, and scalability

## ðŸš€ Running Tests

### Quick Start

```bash
# Run all tests (excluding performance)
python tests/run_tests.py all

# Run unit tests only
python tests/run_tests.py unit

# Run integration tests
python tests/run_tests.py integration

# Run performance tests
python tests/run_tests.py performance

# Quick smoke test
python tests/run_tests.py smoke
```

### Advanced Usage

```bash
# Run with verbose output
python tests/run_tests.py all -v

# Include performance tests in full run
python tests/run_tests.py all -p

# Run specific test pattern
python tests/run_tests.py specific -k "test_user_level"

# Run with coverage analysis
python tests/run_tests.py coverage

# Run without final report
python tests/run_tests.py unit --no-report
```

### Using pytest directly

```bash
# Run all tests
pytest

# Run specific test file
pytest tests/test_user_state_tracker.py

# Run with markers
pytest -m "unit"
pytest -m "integration"
pytest -m "performance"

# Run with coverage
pytest --cov=src/heal/components/onboarding --cov-report=html

# Run specific test
pytest tests/test_user_state_tracker.py::TestUserStateTrackerCore::test_initialization
```

## ðŸ“Š Test Categories and Markers

### Markers
- `unit`: Individual component tests
- `integration`: System interaction tests
- `performance`: Performance and benchmark tests
- `slow`: Long-running tests
- `ui`: Tests requiring UI components
- `smoke`: Quick functionality verification
- `stress`: System limit tests
- `memory`: Memory usage tests
- `concurrent`: Thread safety tests

### Test Organization

```
tests/
â”œâ”€â”€ conftest.py                    # Shared fixtures and configuration
â”œâ”€â”€ run_tests.py                   # Comprehensive test runner
â”œâ”€â”€ test_user_state_tracker.py     # User state management tests
â”œâ”€â”€ test_smart_tip_system.py       # Smart tip system tests
â”œâ”€â”€ test_recommendation_engine.py  # Recommendation engine tests
â”œâ”€â”€ test_tutorial_system.py        # Tutorial system tests
â”œâ”€â”€ test_onboarding_system.py      # System integration tests
â”œâ”€â”€ test_integration_scenarios.py  # Real-world scenario tests
â””â”€â”€ test_performance_benchmarks.py # Performance and stress tests
```

## ðŸ§ª Test Coverage

### Component Coverage

| Component | Unit Tests | Integration Tests | Performance Tests |
|-----------|------------|-------------------|-------------------|
| UserStateTracker | âœ… Complete | âœ… Complete | âœ… Complete |
| SmartTipSystem | âœ… Complete | âœ… Complete | âœ… Complete |
| RecommendationEngine | âœ… Complete | âœ… Complete | âœ… Complete |
| TutorialSystem | âœ… Complete | âœ… Complete | âœ… Complete |
| ContextualHelp | âœ… Complete | âœ… Complete | âœ… Complete |
| FeatureDiscovery | âœ… Complete | âœ… Complete | âœ… Complete |
| DocumentationIntegration | âœ… Complete | âœ… Complete | âœ… Complete |
| OnboardingManager | âœ… Complete | âœ… Complete | âœ… Complete |

### Scenario Coverage

- âœ… First-time user journey
- âœ… Returning user experience
- âœ… User level progression
- âœ… Error handling and recovery
- âœ… Performance under load
- âœ… Memory usage optimization
- âœ… Concurrent operations
- âœ… System integration
- âœ… Configuration management
- âœ… Preference propagation

## ðŸ“ˆ Performance Benchmarks

### Expected Performance Metrics

| Operation | Target Performance | Measured |
|-----------|-------------------|----------|
| System Initialization | < 2.0s | âœ… |
| Action Tracking | > 100 actions/sec | âœ… |
| Recommendation Generation | < 1.0s | âœ… |
| Documentation Search | > 10 searches/sec | âœ… |
| Statistics Generation | < 0.5s | âœ… |

### Memory Usage

| Scenario | Target | Measured |
|----------|--------|----------|
| Baseline Object Count | < 1000 objects | âœ… |
| Memory Leak Detection | < 500 object growth | âœ… |
| Large Data Handling | < 5.0s for 1MB | âœ… |

## ðŸ”§ Test Configuration

### Environment Variables

- `RUN_PERFORMANCE_TESTS=1`: Enable performance tests in CI
- `DISPLAY`: Required for UI tests on Linux
- `CI`: Detected automatically, affects test behavior

### Dependencies

```bash
# Core testing
pip install pytest pytest-cov pytest-mock

# Performance testing
pip install pytest-benchmark memory-profiler

# UI testing (if needed)
pip install pytest-qt

# Coverage reporting
pip install coverage
```

### Configuration Files

- `pytest.ini`: Main pytest configuration
- `conftest.py`: Shared fixtures and test utilities
- `.coveragerc`: Coverage analysis configuration

## ðŸ› Debugging Tests

### Common Issues

1. **Import Errors**: Ensure `src/` is in Python path
2. **Qt Application Issues**: Use `qapp` fixture for UI tests
3. **Mock Failures**: Check mock configuration in `conftest.py`
4. **Performance Test Failures**: May need adjustment for slower systems

### Debug Commands

```bash
# Run with debug output
pytest -s -vv tests/test_specific.py

# Run single test with debugging
pytest --pdb tests/test_file.py::test_function

# Show test output
pytest -s tests/test_file.py

# Run with coverage and show missing lines
pytest --cov=src --cov-report=term-missing
```

## ðŸ“ Writing New Tests

### Test Structure

```python
class TestNewFeature:
    """Test new feature functionality"""
    
    @pytest.fixture
    def setup_feature(self):
        """Setup fixture for feature tests"""
        # Setup code
        yield feature_instance
        # Cleanup code
    
    def test_feature_initialization(self, setup_feature):
        """Test feature initialization"""
        assert setup_feature is not None
        # Test assertions
    
    @pytest.mark.unit
    def test_feature_behavior(self, setup_feature):
        """Test specific feature behavior"""
        # Test implementation
        pass
    
    @pytest.mark.integration
    def test_feature_integration(self, setup_feature):
        """Test feature integration with other components"""
        # Integration test implementation
        pass
```

### Best Practices

1. **Use Descriptive Names**: Test names should clearly describe what is being tested
2. **Arrange-Act-Assert**: Structure tests with clear setup, execution, and verification
3. **Use Fixtures**: Leverage pytest fixtures for common setup and teardown
4. **Mock External Dependencies**: Use mocks to isolate components under test
5. **Test Edge Cases**: Include tests for error conditions and boundary cases
6. **Performance Considerations**: Add performance markers for slow tests
7. **Documentation**: Include docstrings explaining test purpose and approach

### Adding Performance Tests

```python
@pytest.mark.performance
def test_new_feature_performance(self, setup_feature):
    """Test performance of new feature"""
    import time
    
    start_time = time.time()
    
    # Perform operation
    for i in range(1000):
        setup_feature.perform_operation()
    
    end_time = time.time()
    duration = end_time - start_time
    
    # Assert performance threshold
    assert duration < 1.0, f"Operation took {duration:.2f}s, expected < 1.0s"
```

## ðŸ“Š Test Reports

### Coverage Reports

```bash
# Generate HTML coverage report
pytest --cov=src --cov-report=html

# View coverage report
open htmlcov/index.html
```

### Performance Reports

Performance tests output timing information and can be used to track performance regressions over time.

### CI Integration

The test suite is designed to work in CI environments:

- Automatic test discovery
- Proper exit codes for CI systems
- Environment-aware test execution
- Performance test gating

## ðŸŽ¯ Test Goals

### Quality Assurance
- âœ… 100% component coverage
- âœ… All user scenarios tested
- âœ… Error conditions handled
- âœ… Performance requirements met

### Reliability
- âœ… Consistent test results
- âœ… Proper test isolation
- âœ… Comprehensive mocking
- âœ… Resource cleanup

### Maintainability
- âœ… Clear test organization
- âœ… Reusable fixtures
- âœ… Good documentation
- âœ… Easy to extend

## ðŸ”„ Continuous Integration

The test suite supports various CI workflows:

```yaml
# Example GitHub Actions workflow
- name: Run Tests
  run: |
    python tests/run_tests.py all -v
    
- name: Run Performance Tests
  run: |
    python tests/run_tests.py performance -v
    
- name: Generate Coverage
  run: |
    python tests/run_tests.py coverage
```

## ðŸ“ž Support

For test-related issues:

1. Check this documentation
2. Review test output and error messages
3. Examine fixture configuration in `conftest.py`
4. Run individual tests for debugging
5. Check environment setup and dependencies

The test suite is designed to be comprehensive, maintainable, and reliable, ensuring the HEAL onboarding system meets all quality and performance requirements.
