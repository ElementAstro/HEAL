#!/usr/bin/env python3
"""
Optimization Validation Script - ä¼˜åŒ–éªŒè¯è„šæœ¬
è¿è¡Œæ€§èƒ½æµ‹è¯•å¹¶ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
"""

import json
import sys
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.heal.common.optimization_validator import global_optimization_validator, benchmark_function
from src.heal.common.memory_optimizer import global_memory_optimizer, optimize_memory
from src.heal.common.workflow_optimizer import create_workflow
from src.heal.common.logging_config import get_logger

logger = get_logger(__name__)


def run_performance_benchmarks():
    """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    logger.info("å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
    
    benchmarks = {}
    
    # 1. å†…å­˜ä¼˜åŒ–åŸºå‡†æµ‹è¯•
    logger.info("æµ‹è¯•å†…å­˜ä¼˜åŒ–æ€§èƒ½")
    memory_benchmark = benchmark_function("memory_optimization", optimize_memory)
    benchmarks["memory_optimization"] = memory_benchmark
    
    # 2. JSONå¤„ç†åŸºå‡†æµ‹è¯•
    logger.info("æµ‹è¯•JSONå¤„ç†æ€§èƒ½")
    
    def json_processing_test():
        from src.heal.common.json_utils import JsonUtils
        import tempfile
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = {
            "config": {"setting1": "value1", "setting2": "value2"},
            "data": list(range(1000)),
            "metadata": {"version": "1.0", "timestamp": time.time()}
        }
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump(test_data, f)
            temp_path = Path(f.name)
        
        try:
            # æµ‹è¯•åŠ è½½
            result = JsonUtils.load_json_file(temp_path)
            
            # æµ‹è¯•ä¿å­˜
            JsonUtils.save_json_file(temp_path, test_data)
            
            return result.success
        finally:
            temp_path.unlink(missing_ok=True)
    
    json_benchmark = benchmark_function("json_processing", json_processing_test, iterations=10)
    benchmarks["json_processing"] = json_benchmark
    
    # 3. ç¼“å­˜ç³»ç»ŸåŸºå‡†æµ‹è¯•
    logger.info("æµ‹è¯•ç¼“å­˜ç³»ç»Ÿæ€§èƒ½")
    
    def cache_performance_test():
        from src.heal.common.cache_manager import global_cache_manager
        
        cache = global_cache_manager.get_cache("benchmark_test")
        if not cache:
            from src.heal.common.cache_manager import LRUCache
            cache = LRUCache(max_size=1000)
            global_cache_manager.register_cache("benchmark_test", cache)
        
        # å†™å…¥æµ‹è¯•
        for i in range(500):
            cache.put(f"key_{i}", f"value_{i}")
        
        # è¯»å–æµ‹è¯•
        results = []
        for i in range(500):
            result = cache.get(f"key_{i}")
            results.append(result)
        
        # æ¸…ç†æµ‹è¯•
        cache.clear()
        
        return len([r for r in results if r is not None])
    
    cache_benchmark = benchmark_function("cache_performance", cache_performance_test, iterations=5)
    benchmarks["cache_performance"] = cache_benchmark
    
    # 4. å·¥ä½œæµç³»ç»ŸåŸºå‡†æµ‹è¯•
    logger.info("æµ‹è¯•å·¥ä½œæµç³»ç»Ÿæ€§èƒ½")
    
    def workflow_performance_test():
        workflow = create_workflow("benchmark_workflow")
        
        def step1():
            time.sleep(0.001)  # 1msæ¨¡æ‹Ÿå·¥ä½œ
            return "step1_done"
        
        def step2():
            time.sleep(0.001)  # 1msæ¨¡æ‹Ÿå·¥ä½œ
            return "step2_done"
        
        def step3():
            time.sleep(0.001)  # 1msæ¨¡æ‹Ÿå·¥ä½œ
            return "step3_done"
        
        workflow.add_step("step1", step1)
        workflow.add_step("step2", step2, dependencies=["step1"])
        workflow.add_step("step3", step3, dependencies=["step2"])
        
        result = workflow.execute()
        return result["success"]
    
    workflow_benchmark = benchmark_function("workflow_performance", workflow_performance_test, iterations=10)
    benchmarks["workflow_performance"] = workflow_benchmark
    
    logger.info("æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ")
    return benchmarks


def validate_optimizations():
    """éªŒè¯ä¼˜åŒ–æ•ˆæœ"""
    logger.info("å¼€å§‹ä¼˜åŒ–éªŒè¯")
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmarks = run_performance_benchmarks()
    
    # è¿è¡Œç»¼åˆéªŒè¯
    validation_summary = global_optimization_validator.run_comprehensive_validation()
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    report = global_optimization_validator.get_validation_report()
    
    return {
        "benchmarks": benchmarks,
        "validation_summary": validation_summary,
        "detailed_report": report,
        "timestamp": time.time()
    }


def generate_optimization_report(results, output_file=None):
    """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
    logger.info("ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š")
    
    report_content = {
        "optimization_validation_report": {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(results["timestamp"])),
            "summary": results["validation_summary"],
            "performance_benchmarks": {},
            "recommendations": [],
            "issues": []
        }
    }
    
    # å¤„ç†åŸºå‡†æµ‹è¯•ç»“æœ
    for name, benchmark in results["benchmarks"].items():
        report_content["optimization_validation_report"]["performance_benchmarks"][name] = {
            "execution_time_ms": benchmark.execution_time * 1000,
            "memory_usage_mb": benchmark.memory_usage_mb,
            "cpu_usage_percent": benchmark.cpu_usage_percent,
            "success": benchmark.success,
            "error": benchmark.error
        }
        
        # ç”Ÿæˆå»ºè®®
        if benchmark.execution_time > 1.0:  # è¶…è¿‡1ç§’
            report_content["optimization_validation_report"]["issues"].append(
                f"{name}: æ‰§è¡Œæ—¶é—´è¿‡é•¿ ({benchmark.execution_time:.2f}s)"
            )
            report_content["optimization_validation_report"]["recommendations"].append(
                f"ä¼˜åŒ– {name} çš„æ‰§è¡Œæ•ˆç‡"
            )
        
        if benchmark.memory_usage_mb > 50:  # è¶…è¿‡50MB
            report_content["optimization_validation_report"]["issues"].append(
                f"{name}: å†…å­˜ä½¿ç”¨è¿‡é«˜ ({benchmark.memory_usage_mb:.1f}MB)"
            )
            report_content["optimization_validation_report"]["recommendations"].append(
                f"ä¼˜åŒ– {name} çš„å†…å­˜ä½¿ç”¨"
            )
    
    # æ·»åŠ é€šç”¨å»ºè®®
    success_rate = results["validation_summary"]["success_rate"]
    if success_rate < 0.8:
        report_content["optimization_validation_report"]["recommendations"].extend([
            "è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½å…³é”®è·¯å¾„",
            "æ£€æŸ¥å†…å­˜æ³„æ¼å’Œèµ„æºç®¡ç†",
            "ä¼˜åŒ–ç¼“å­˜ç­–ç•¥å’Œå‘½ä¸­ç‡",
            "è€ƒè™‘ä½¿ç”¨æ›´å¤šå¼‚æ­¥æ“ä½œ"
        ])
    elif success_rate >= 0.9:
        report_content["optimization_validation_report"]["recommendations"].append(
            "ä¼˜åŒ–æ•ˆæœè‰¯å¥½ï¼Œç»§ç»­ç›‘æ§æ€§èƒ½æŒ‡æ ‡"
        )
    
    # ä¿å­˜æŠ¥å‘Š
    if output_file:
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report_content, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
    
    return report_content


def print_summary(results):
    """æ‰“å°ä¼˜åŒ–æ‘˜è¦"""
    summary = results["validation_summary"]
    
    print("\n" + "="*60)
    print("HEAL é¡¹ç›®ä¼˜åŒ–éªŒè¯æŠ¥å‘Š")
    print("="*60)
    
    print(f"\nğŸ“Š éªŒè¯ç»Ÿè®¡:")
    print(f"  æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
    print(f"  é€šè¿‡æµ‹è¯•: {summary['passed_tests']}")
    print(f"  å¤±è´¥æµ‹è¯•: {summary['failed_tests']}")
    print(f"  æˆåŠŸç‡: {summary['success_rate']:.1%}")
    
    print(f"\nâš¡ æ€§èƒ½åŸºå‡†:")
    for name, benchmark in results["benchmarks"].items():
        status = "âœ…" if benchmark.success else "âŒ"
        print(f"  {status} {name}:")
        print(f"    æ‰§è¡Œæ—¶é—´: {benchmark.execution_time*1000:.1f}ms")
        print(f"    å†…å­˜ä½¿ç”¨: {benchmark.memory_usage_mb:.1f}MB")
        if benchmark.error:
            print(f"    é”™è¯¯: {benchmark.error}")
    
    if summary.get("issues_found"):
        print(f"\nâš ï¸  å‘ç°çš„é—®é¢˜:")
        for issue in summary["issues_found"]:
            print(f"  â€¢ {issue}")
    
    if summary.get("recommendations"):
        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for rec in summary["recommendations"]:
            print(f"  â€¢ {rec}")
    
    print("\n" + "="*60)


def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¼€å§‹HEALé¡¹ç›®ä¼˜åŒ–éªŒè¯")
    
    try:
        # è¿è¡ŒéªŒè¯
        results = validate_optimizations()
        
        # ç”ŸæˆæŠ¥å‘Š
        report_file = project_root / "optimization_report.json"
        generate_optimization_report(results, report_file)
        
        # æ‰“å°æ‘˜è¦
        print_summary(results)
        
        # è¿”å›æˆåŠŸç‡ä½œä¸ºé€€å‡ºç 
        success_rate = results["validation_summary"]["success_rate"]
        if success_rate >= 0.8:
            logger.info("ä¼˜åŒ–éªŒè¯æˆåŠŸå®Œæˆ")
            return 0
        else:
            logger.warning(f"ä¼˜åŒ–éªŒè¯å®Œæˆï¼Œä½†æˆåŠŸç‡è¾ƒä½: {success_rate:.1%}")
            return 1
            
    except Exception as e:
        logger.error(f"ä¼˜åŒ–éªŒè¯å¤±è´¥: {e}")
        print(f"\nâŒ éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return 2


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
